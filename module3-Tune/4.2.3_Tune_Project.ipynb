{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "nteract": {
      "version": "0.22.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGGrt9EYlCqY"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# BloomTech - Hyperparameter Tuning Project\n",
        "\n",
        "## Data Science Unit 4.2.3\n",
        "\n",
        "Michael Luo\n",
        "\n",
        "2023/03/01\n",
        "\n",
        "# Gridsearch Hyperparameters\n",
        "\n",
        "In the guided project, you learned how to use sklearn's `GridsearchCV` and `keras-tuner` libraries to tune the hyperparameters of a neural network model. For your module project you'll continue using these two libraries, however we are going to make things a little more interesting for you. \n",
        "\n",
        "Continue to use TensorFlow Keras & a sample of the [Quickdraw dataset](https://github.com/googlecreativelab/quickdraw-dataset) to build a sketch classification model. The dataset has been sampled to only 10 classes and 10000 observations per class. \n",
        "\n",
        "\n",
        "\n",
        "**Don't forget to switch to GPU on Colab!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7oEgGCV3_hY"
      },
      "source": [
        "## 0.1 Imports and installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxctNMPb7mNY"
      },
      "source": [
        "# native python libraries imports \n",
        "import math\n",
        "from time import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# sklearn imports \n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# keras imports \n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.activations import relu, sigmoid\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.utils import get_file\n",
        "\n",
        "# required for compatibility between sklearn and keras\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "# install keras-tuner\n",
        "!pip install keras-tuner\n",
        "# from kerastuner.tuners import RandomSearch, BayesianOptimization, Sklearn ??\n",
        "from kerastuner.tuners import RandomSearch, BayesianOptimization\n",
        "from kerastuner.engine.hyperparameters import HyperParameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMBS8CRBzYqB"
      },
      "source": [
        "## 0.2 Load quickdraw data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr8w6IX37mNa"
      },
      "source": [
        "def load_quickdraw10():\n",
        "    \"\"\"\n",
        "    Loads and normalizes (based on max pixel value) and partitions a subset of \n",
        "    the QuickDraw dataset into training and test sets.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X, y : shuffled train test split\n",
        "    \"\"\"\n",
        "    \n",
        "    URL_ = \"https://github.com/michael-s-luo/Neural-Networks-DS4.2/blob/main/data/quickdraw10.npz?raw=true\"\n",
        "    \n",
        "    path_to_zip = get_file('./quickdraw10.npz', origin=URL_, extract=False)\n",
        "\n",
        "    data = np.load(path_to_zip)\n",
        "    \n",
        "    # normalize your image data\n",
        "    max_pixel_value = 255\n",
        "    X = data['arr_0']/max_pixel_value\n",
        "    Y = data['arr_1']\n",
        "        \n",
        "    return train_test_split(X, Y, shuffle=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjU5nY3e7mNc"
      },
      "source": [
        "X_train, X_test, y_train, y_test = load_quickdraw10()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkvBPoUy7mNd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e3de136-45f4-464c-93ad-7e6fc9afa5e2"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(75000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4dx6VA07mNe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25b7efd7-da07-41a9-a486-9acb54af8a6e"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(75000,)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXsWtj8Z7mNf"
      },
      "source": [
        "_____\n",
        "\n",
        "# Experiment 1\n",
        "\n",
        "## Tune Hyperperameters using Enhanced GridsearchCV \n",
        "\n",
        "We are going to use GridsearchCV again to tune a deep learning model however we are going to add some additional functionality to our gridsearch. \n",
        "\n",
        "Specifically, we are going to automate the generation of how many nodes to use in a layer and how many layers to use in a model! \n",
        "\n",
        "By the way, yes, there is a function within a function. Try to not let that bother you. An alternative to this would be to create a class. If you're up for the challenge give it a shot. However, consider this a stretch goal that you come back to after you finish going through this assignment. \n",
        "\n",
        "\n",
        "### Objective \n",
        "\n",
        "The objective of this experiment is to show you how to automate the generation of layers and layer nodes for the purposes of gridsearch. <br>\n",
        "Up until now, we've been manually selecting the number of layers and layer nodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USXjs7Hk71Hy"
      },
      "source": [
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(n_layers,  first_layer_nodes, last_layer_nodes, act_funct =\"relu\", negative_node_incrementation=True):\n",
        "    \"\"\"\"\n",
        "    Returns a compiled keras model \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    n_layers: int \n",
        "        number of hidden layers in model \n",
        "        To be clear, this excludes the input and output layer.\n",
        "        \n",
        "    first_layer_nodes: int\n",
        "        Number of nodes in the first hidden layer \n",
        "\n",
        "    last_layer_nodes: int\n",
        "        Number of nodes in the last hidden layer (this is the layer just prior to the output layer)\n",
        "        \n",
        "     act_funct: string \n",
        "         Name of activation function to use in hidden layers (this excludes the output layer)\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    model: keras object \n",
        "    \"\"\"\n",
        "    \n",
        "    def gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation=True):\n",
        "        \"\"\"\n",
        "        Generates and returns the number of nodes in each hidden layer. \n",
        "        To be clear, this excludes the input and output layer. \n",
        "\n",
        "        Note\n",
        "        ----\n",
        "        Number of nodes in each layer is linearly incremented. \n",
        "        For example, gen_layer_nodes(5, 500, 100) will generate [500, 400, 300, 200, 100]\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_layers: int\n",
        "            Number of hidden layers\n",
        "            This values should be 2 or greater \n",
        "\n",
        "        first_layer_nodes: int\n",
        "\n",
        "        last_layer_nodes: int\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        layers: list of ints\n",
        "            Contains number of nodes for each layer \n",
        "        \"\"\"\n",
        "\n",
        "        # throws an error if n_layers is less than 2 \n",
        "        assert n_layers >= 2, \"n_layers must be 2 or greater\"\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        # PROTIP: IF YOU WANT THE NODE INCREMENTATION TO BE SPACED DIFFERENTLY\n",
        "        # THEN YOU'LL NEED TO CHANGE THE WAY THAT IT'S CALCULATED \n",
        "        # when set to True number of nodes are decreased for subsequent layers \n",
        "        # NOTE: the order of the number of nodes doesn't matter\n",
        "        if negative_node_incrementation:\n",
        "            # subtract this amount from previous layer's nodes in order to increment towards smaller numbers \n",
        "            nodes_increment = (last_layer_nodes - first_layer_nodes)/ (n_layers-1)\n",
        "            \n",
        "        # when set to False number of nodes are increased for subsequent layers\n",
        "        else:\n",
        "            # add this amount from previous layer's nodes in order to increment towards larger numbers \n",
        "            nodes_increment = (last_layer_nodes - first_layer_nodes)/ (n_layers-1)\n",
        "\n",
        "        nodes = first_layer_nodes\n",
        "\n",
        "        for i in range(1, n_layers+1):\n",
        "\n",
        "            layers.append(nodes)\n",
        "\n",
        "            # increment nodes for next layer \n",
        "            nodes = nodes + nodes_increment\n",
        "\n",
        "        return np.round(layers)\n",
        "    \n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    \n",
        "    n_nodes = gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation)\n",
        "    \n",
        "    for i in range(0, n_layers):\n",
        "        if i==0:\n",
        "            model.add(Dense(first_layer_nodes, input_dim=X_train.shape[1], activation=act_funct))\n",
        "        else:\n",
        "            model.add(Dense(n_nodes[i], activation=act_funct))\n",
        "            \n",
        "    # output layer \n",
        "    model.add(Dense(10, # 10 unit/neurons in output layer because we have 10 possible labels to predict  \n",
        "                    activation='softmax')) # use softmax for a label set greater than 2            \n",
        "    \n",
        "    # Compile model\n",
        "    model.compile(loss='sparse_categorical_crossentropy', \n",
        "                  optimizer='adam', # adam is a good default optimizer \n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    # do not include model.fit() inside the create_model function\n",
        "    # KerasClassifier is expecting a complied model \n",
        "    return model\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YO-x0nqt7mNh"
      },
      "source": [
        "## 1.1 Explore `create_model`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The helper function `gen_layer_nodes()` which is contained inside `create_model()` <br>\n",
        "returns a list containing the number of nodes for each successive layer.<br>\n",
        "\n",
        "Let's check that `gen_layer_nodes()` behaves as expected. <br>\n",
        "In other words, we'll perform a **Unit Test!**"
      ],
      "metadata": {
        "id": "-1hnjQHKW19w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation=True):\n",
        "        \"\"\"\n",
        "        Generates and returns the number of nodes in each hidden layer. \n",
        "        To be clear, this excludes the input and output layer. \n",
        "\n",
        "        Note\n",
        "        ----\n",
        "        Number of nodes in each layer is linearly incremented. \n",
        "        For example, gen_layer_nodes(5, 500, 100) will generate [500, 400, 300, 200, 100]\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_layers: int\n",
        "            Number of hidden layers\n",
        "            This values should be 2 or greater \n",
        "\n",
        "        first_layer_nodes: int\n",
        "\n",
        "        last_layer_nodes: int\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        layers: list of ints\n",
        "            Contains number of nodes for each layer \n",
        "        \"\"\"\n",
        "\n",
        "        # throws an error if n_layers is less than 2 \n",
        "        assert n_layers >= 2, \"n_layers must be 2 or greater\"\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        # PROTIP: IF YOU WANT THE NODE INCREMENTATION TO BE SPACED DIFFERENTLY\n",
        "        # THEN YOU'LL NEED TO CHANGE THE WAY THAT IT'S CALCULATED - HAVE FUN!\n",
        "        # when set to True number of nodes are decreased for subsequent layers \n",
        "        # NOTE: the order of the number of nodes doesn't matter\n",
        "        if negative_node_incrementation:\n",
        "            # subtract this amount from previous layer's nodes in order to increment towards smaller numbers \n",
        "            nodes_increment = (last_layer_nodes - first_layer_nodes)/ (n_layers-1)\n",
        "            #print(f'nodes increment = {nodes_increment}')\n",
        "            \n",
        "        # when set to False number of nodes are increased for subsequent layers\n",
        "        else:\n",
        "            # add this amount from previous layer's nodes in order to increment towards larger numbers \n",
        "            nodes_increment = (last_layer_nodes - first_layer_nodes)/ (n_layers-1)\n",
        "            #print(f'nodes increment = {nodes_increment}')\n",
        "\n",
        "        nodes = first_layer_nodes\n",
        "\n",
        "        for i in range(1, n_layers+1):\n",
        "\n",
        "            layers.append(nodes)\n",
        "\n",
        "            # increment nodes for next layer \n",
        "            nodes = nodes + nodes_increment\n",
        "\n",
        "        return np.round(layers)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YiPXu0p_Qco_"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `negative_node_incrementation = True`\n",
        "For this case we want the number of nodes to _decrease_ by a constant number for successive layers. <br>So `first_layer_nodes` must be _larger_ than `last_layer_nodes` "
      ],
      "metadata": {
        "id": "Mj3MrB6jXUMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function was provided by the BloomTech curriculum.\n",
        "# negative_node_incrementation has no effect on the n_nodes outcome (which is a list of number of nodes per layer in succession)\n",
        "  # The function always subtracts first_layer_nodes from last_layer_nodes; already outputs correct directional increment\n",
        "\n",
        "# n_layers = 10\n",
        "# first_layer_nodes = 100\n",
        "# last_layer_nodes = 500\n",
        "# negative_node_incrementation = True\n",
        "# n_nodes = gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation)\n",
        "# print(f'Number of nodes in successive layers: {n_nodes}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4m4jRNllXPPG",
        "outputId": "94f793ec-913c-46e5-d6ce-ebc2a6b4d402"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes in successive layers: [100. 144. 189. 233. 278. 322. 367. 411. 456. 500.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `negative_node_incrementation = False`\n",
        "For this case we want the number of nodes to _increase_ by a constant number for successive layers. <br>So `first_layer_nodes` must be _smaller_ than `last_layer_nodes` "
      ],
      "metadata": {
        "id": "ttkaf3g9XhGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function was provided by the BloomTech curriculum.\n",
        "# negative_node_incrementation has no effect on the n_nodes outcome (which is a list of number of nodes per layer in succession)\n",
        "  # The function always subtracts first_layer_nodes from last_layer_nodes; already outputs correct directional increment\n",
        "\n",
        "\n",
        "# n_layers = 5\n",
        "# first_layer_nodes = 100\n",
        "# last_layer_nodes = 500\n",
        "# negative_node_incrementation = False\n",
        "# n_nodes = gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation)\n",
        "# print(f'Number of nodes in successive layers: {n_nodes}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fkrMS8bXQUo",
        "outputId": "9fa80cef-84b8-4864-c34e-04a970421163"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes in successive layers: [100, 200, 300, 400, 500]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OK, the Unit Test is passed!"
      ],
      "metadata": {
        "id": "FHuB-bm5Wkpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's build a few models<br> \n",
        "in order to understand how `create_model()` works in practice. "
      ],
      "metadata": {
        "id": "qO3AjVWOZ6SA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95E85Ug07mNh"
      },
      "source": [
        "### Build a model, setting `negative_node_incrementation = True` \n",
        "\n",
        "Use `create_model` to build a model. \n",
        "\n",
        "- Set `n_layers = 10` \n",
        "- Set `first_layer_nodes = 500`\n",
        "- Set `last_layer_nodes = 100`\n",
        "- Set `act_funct = \"relu\"`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5dcf5c585f07629a03086cf57ba53615",
          "grade": false,
          "grade_id": "cell-86d63e89a21223de",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "x_1REOCY7mNi"
      },
      "source": [
        "# use create_model to create a model \n",
        "\n",
        "# YOUR CODE HERE\n",
        "model = create_model(n_layers = 10, first_layer_nodes = 500, last_layer_nodes = 100)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_config()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFiw1OBG93Rc",
        "outputId": "5c4a924a-f546-4402-9ad1-8a52b1abea50"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'sequential_4',\n",
              " 'layers': [{'class_name': 'InputLayer',\n",
              "   'config': {'batch_input_shape': (None, 784),\n",
              "    'dtype': 'float32',\n",
              "    'sparse': False,\n",
              "    'ragged': False,\n",
              "    'name': 'dense_26_input'}},\n",
              "  {'class_name': 'Dense',\n",
              "   'config': {'name': 'dense_26',\n",
              "    'trainable': True,\n",
              "    'dtype': 'float32',\n",
              "    'batch_input_shape': (None, 784),\n",
              "    'units': 500,\n",
              "    'activation': 'relu',\n",
              "    'use_bias': True,\n",
              "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
              "     'config': {'seed': None}},\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'kernel_regularizer': None,\n",
              "    'bias_regularizer': None,\n",
              "    'activity_regularizer': None,\n",
              "    'kernel_constraint': None,\n",
              "    'bias_constraint': None}},\n",
              "  {'class_name': 'Dense',\n",
              "   'config': {'name': 'dense_27',\n",
              "    'trainable': True,\n",
              "    'dtype': 'float32',\n",
              "    'units': 456,\n",
              "    'activation': 'relu',\n",
              "    'use_bias': True,\n",
              "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
              "     'config': {'seed': None}},\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'kernel_regularizer': None,\n",
              "    'bias_regularizer': None,\n",
              "    'activity_regularizer': None,\n",
              "    'kernel_constraint': None,\n",
              "    'bias_constraint': None}},\n",
              "  {'class_name': 'Dense',\n",
              "   'config': {'name': 'dense_28',\n",
              "    'trainable': True,\n",
              "    'dtype': 'float32',\n",
              "    'units': 412,\n",
              "    'activation': 'relu',\n",
              "    'use_bias': True,\n",
              "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
              "     'config': {'seed': None}},\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'kernel_regularizer': None,\n",
              "    'bias_regularizer': None,\n",
              "    'activity_regularizer': None,\n",
              "    'kernel_constraint': None,\n",
              "    'bias_constraint': None}},\n",
              "  {'class_name': 'Dense',\n",
              "   'config': {'name': 'dense_29',\n",
              "    'trainable': True,\n",
              "    'dtype': 'float32',\n",
              "    'units': 367,\n",
              "    'activation': 'relu',\n",
              "    'use_bias': True,\n",
              "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
              "     'config': {'seed': None}},\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'kernel_regularizer': None,\n",
              "    'bias_regularizer': None,\n",
              "    'activity_regularizer': None,\n",
              "    'kernel_constraint': None,\n",
              "    'bias_constraint': None}},\n",
              "  {'class_name': 'Dense',\n",
              "   'config': {'name': 'dense_30',\n",
              "    'trainable': True,\n",
              "    'dtype': 'float32',\n",
              "    'units': 323,\n",
              "    'activation': 'relu',\n",
              "    'use_bias': True,\n",
              "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
              "     'config': {'seed': None}},\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'kernel_regularizer': None,\n",
              "    'bias_regularizer': None,\n",
              "    'activity_regularizer': None,\n",
              "    'kernel_constraint': None,\n",
              "    'bias_constraint': None}},\n",
              "  {'class_name': 'Dense',\n",
              "   'config': {'name': 'dense_31',\n",
              "    'trainable': True,\n",
              "    'dtype': 'float32',\n",
              "    'units': 278,\n",
              "    'activation': 'relu',\n",
              "    'use_bias': True,\n",
              "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
              "     'config': {'seed': None}},\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'kernel_regularizer': None,\n",
              "    'bias_regularizer': None,\n",
              "    'activity_regularizer': None,\n",
              "    'kernel_constraint': None,\n",
              "    'bias_constraint': None}},\n",
              "  {'class_name': 'Dense',\n",
              "   'config': {'name': 'dense_32',\n",
              "    'trainable': True,\n",
              "    'dtype': 'float32',\n",
              "    'units': 234,\n",
              "    'activation': 'relu',\n",
              "    'use_bias': True,\n",
              "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
              "     'config': {'seed': None}},\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'kernel_regularizer': None,\n",
              "    'bias_regularizer': None,\n",
              "    'activity_regularizer': None,\n",
              "    'kernel_constraint': None,\n",
              "    'bias_constraint': None}},\n",
              "  {'class_name': 'Dense',\n",
              "   'config': {'name': 'dense_33',\n",
              "    'trainable': True,\n",
              "    'dtype': 'float32',\n",
              "    'units': 189,\n",
              "    'activation': 'relu',\n",
              "    'use_bias': True,\n",
              "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
              "     'config': {'seed': None}},\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'kernel_regularizer': None,\n",
              "    'bias_regularizer': None,\n",
              "    'activity_regularizer': None,\n",
              "    'kernel_constraint': None,\n",
              "    'bias_constraint': None}},\n",
              "  {'class_name': 'Dense',\n",
              "   'config': {'name': 'dense_34',\n",
              "    'trainable': True,\n",
              "    'dtype': 'float32',\n",
              "    'units': 145,\n",
              "    'activation': 'relu',\n",
              "    'use_bias': True,\n",
              "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
              "     'config': {'seed': None}},\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'kernel_regularizer': None,\n",
              "    'bias_regularizer': None,\n",
              "    'activity_regularizer': None,\n",
              "    'kernel_constraint': None,\n",
              "    'bias_constraint': None}},\n",
              "  {'class_name': 'Dense',\n",
              "   'config': {'name': 'dense_35',\n",
              "    'trainable': True,\n",
              "    'dtype': 'float32',\n",
              "    'units': 100,\n",
              "    'activation': 'relu',\n",
              "    'use_bias': True,\n",
              "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
              "     'config': {'seed': None}},\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'kernel_regularizer': None,\n",
              "    'bias_regularizer': None,\n",
              "    'activity_regularizer': None,\n",
              "    'kernel_constraint': None,\n",
              "    'bias_constraint': None}},\n",
              "  {'class_name': 'Dense',\n",
              "   'config': {'name': 'dense_36',\n",
              "    'trainable': True,\n",
              "    'dtype': 'float32',\n",
              "    'units': 10,\n",
              "    'activation': 'softmax',\n",
              "    'use_bias': True,\n",
              "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
              "     'config': {'seed': None}},\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'kernel_regularizer': None,\n",
              "    'bias_regularizer': None,\n",
              "    'activity_regularizer': None,\n",
              "    'kernel_constraint': None,\n",
              "    'bias_constraint': None}}]}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYMwZQ7k7mNi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56f3ee85-d3bd-4c0a-d98f-ad54dc543fc4"
      },
      "source": [
        "# run model.summary() and make sure that you understand the model architecture that you just built \n",
        "# Notice in the model summary how the number of nodes have been linearly incremented in decreasing values. \n",
        "model.summary()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_26 (Dense)            (None, 500)               392500    \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 456)               228456    \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 412)               188284    \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 367)               151571    \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 323)               118864    \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 278)               90072     \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 234)               65286     \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 189)               44415     \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 145)               27550     \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 100)               14600     \n",
            "                                                                 \n",
            " dense_36 (Dense)            (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,322,608\n",
            "Trainable params: 1,322,608\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUc0jfnRm-uh"
      },
      "source": [
        "### Build a model, setting `negative_node_incrementation = False` \n",
        "\n",
        "Use `create_model` to build a model. \n",
        "\n",
        "- Set `n_layers = 10` \n",
        "- Set `first_layer_nodes = 100`\n",
        "- Set `last_layer_nodes = 500`\n",
        "- Set `act_funct = \"relu\"`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5dcf5c585f07629a03086cf57ba53615",
          "grade": false,
          "grade_id": "cell-86d63e89a21223de",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "3_-kqHQtm-ui"
      },
      "source": [
        "# use create_model to create a model \n",
        "\n",
        "model = create_model(n_layers=10, first_layer_nodes=100, last_layer_nodes=500)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piboKWsNm-uj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1be2914e-78c0-4e8c-df7c-a9f27cc47e23"
      },
      "source": [
        "# run model.summary() and make sure that you understand the model architecture that you just built \n",
        "# Notice in the model summary how the number of nodes have been linearly incremented in decreasing values. \n",
        "model.summary()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_59 (Dense)            (None, 100)               78500     \n",
            "                                                                 \n",
            " dense_60 (Dense)            (None, 144)               14544     \n",
            "                                                                 \n",
            " dense_61 (Dense)            (None, 189)               27405     \n",
            "                                                                 \n",
            " dense_62 (Dense)            (None, 233)               44270     \n",
            "                                                                 \n",
            " dense_63 (Dense)            (None, 278)               65052     \n",
            "                                                                 \n",
            " dense_64 (Dense)            (None, 322)               89838     \n",
            "                                                                 \n",
            " dense_65 (Dense)            (None, 367)               118541    \n",
            "                                                                 \n",
            " dense_66 (Dense)            (None, 411)               151248    \n",
            "                                                                 \n",
            " dense_67 (Dense)            (None, 456)               187872    \n",
            "                                                                 \n",
            " dense_68 (Dense)            (None, 500)               228500    \n",
            "                                                                 \n",
            " dense_69 (Dense)            (None, 10)                5010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,010,780\n",
            "Trainable params: 1,010,780\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBH7AR9p0OXi"
      },
      "source": [
        "## 1.2 Create a grid search using `sklearn`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veloj7Nnlttf"
      },
      "source": [
        "### Hyperparameter search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e2lhZqP7mNn"
      },
      "source": [
        "# define the grid search parameters\n",
        "param_grid = {'n_layers': [2, 3],\n",
        "              'epochs': [3], \n",
        "              \"first_layer_nodes\": [500, 300],\n",
        "              \"last_layer_nodes\": [100, 50]\n",
        "             }"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ks_MLPB7mNn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "940d811f-8bcd-4461-8bc4-30386ffbf96d"
      },
      "source": [
        "model = KerasClassifier(build_fn=create_model)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-70-d5ce3444c54d>:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(build_fn=create_model)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8GKbLJ_7mNn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f26bc2f3-611c-431d-bc3f-6acb42a2b01c"
      },
      "source": [
        "%%time\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, \n",
        "                    param_grid=param_grid, \n",
        "                    n_jobs=-2, \n",
        "                    verbose=2, \n",
        "                    cv=3)\n",
        "\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 10s 3ms/step - loss: 0.6429 - accuracy: 0.8060\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4345 - accuracy: 0.8690\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3490 - accuracy: 0.8943\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4982 - accuracy: 0.8552\n",
            "[CV] END epochs=3, first_layer_nodes=500, last_layer_nodes=100, n_layers=2; total time=  24.4s\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6454 - accuracy: 0.8068\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4344 - accuracy: 0.8684\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3488 - accuracy: 0.8931\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4479 - accuracy: 0.8674\n",
            "[CV] END epochs=3, first_layer_nodes=500, last_layer_nodes=100, n_layers=2; total time=  24.5s\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6437 - accuracy: 0.8050\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4345 - accuracy: 0.8664\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3500 - accuracy: 0.8919\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4631 - accuracy: 0.8652\n",
            "[CV] END epochs=3, first_layer_nodes=500, last_layer_nodes=100, n_layers=2; total time=  20.9s\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6383 - accuracy: 0.8055\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4404 - accuracy: 0.8652\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3537 - accuracy: 0.8898\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.4547 - accuracy: 0.8681\n",
            "[CV] END epochs=3, first_layer_nodes=500, last_layer_nodes=100, n_layers=3; total time=  21.3s\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 3ms/step - loss: 0.6442 - accuracy: 0.8011\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4392 - accuracy: 0.8657\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3554 - accuracy: 0.8908\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4413 - accuracy: 0.8679\n",
            "[CV] END epochs=3, first_layer_nodes=500, last_layer_nodes=100, n_layers=3; total time=  21.2s\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6433 - accuracy: 0.8011\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4393 - accuracy: 0.8645\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3567 - accuracy: 0.8901\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4642 - accuracy: 0.8644\n",
            "[CV] END epochs=3, first_layer_nodes=500, last_layer_nodes=100, n_layers=3; total time=  21.9s\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6500 - accuracy: 0.8037\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4407 - accuracy: 0.8668\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3551 - accuracy: 0.8911\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4407 - accuracy: 0.8719\n",
            "[CV] END epochs=3, first_layer_nodes=500, last_layer_nodes=50, n_layers=2; total time=  20.2s\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6581 - accuracy: 0.8019\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4460 - accuracy: 0.8660\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3589 - accuracy: 0.8893\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4566 - accuracy: 0.8672\n",
            "[CV] END epochs=3, first_layer_nodes=500, last_layer_nodes=50, n_layers=2; total time=  19.9s\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 3ms/step - loss: 0.6512 - accuracy: 0.8000\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4460 - accuracy: 0.8638\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3578 - accuracy: 0.8906\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4539 - accuracy: 0.8686\n",
            "[CV] END epochs=3, first_layer_nodes=500, last_layer_nodes=50, n_layers=2; total time=  20.7s\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 8s 4ms/step - loss: 0.6531 - accuracy: 0.7995\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4393 - accuracy: 0.8649\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3577 - accuracy: 0.8892\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.4436 - accuracy: 0.8685\n",
            "[CV] END epochs=3, first_layer_nodes=500, last_layer_nodes=50, n_layers=3; total time=  28.7s\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 11s 4ms/step - loss: 0.6510 - accuracy: 0.7993\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4448 - accuracy: 0.8650\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3594 - accuracy: 0.8896\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.4732 - accuracy: 0.8556\n",
            "[CV] END epochs=3, first_layer_nodes=500, last_layer_nodes=50, n_layers=3; total time=  28.1s\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 3ms/step - loss: 0.6479 - accuracy: 0.7986\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4435 - accuracy: 0.8632\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3578 - accuracy: 0.8892\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4629 - accuracy: 0.8641\n",
            "[CV] END epochs=3, first_layer_nodes=500, last_layer_nodes=50, n_layers=3; total time=  21.4s\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6629 - accuracy: 0.7980\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4552 - accuracy: 0.8617\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3690 - accuracy: 0.8864\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4493 - accuracy: 0.8670\n",
            "[CV] END epochs=3, first_layer_nodes=300, last_layer_nodes=100, n_layers=2; total time=  24.5s\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6631 - accuracy: 0.8004\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4501 - accuracy: 0.8639\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3687 - accuracy: 0.8874\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4640 - accuracy: 0.8626\n",
            "[CV] END epochs=3, first_layer_nodes=300, last_layer_nodes=100, n_layers=2; total time=  20.5s\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6655 - accuracy: 0.7988\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4559 - accuracy: 0.8627\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3718 - accuracy: 0.8865\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4634 - accuracy: 0.8636\n",
            "[CV] END epochs=3, first_layer_nodes=300, last_layer_nodes=100, n_layers=2; total time=  24.5s\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 3ms/step - loss: 0.6550 - accuracy: 0.7986\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4495 - accuracy: 0.8633\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3684 - accuracy: 0.8871\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4923 - accuracy: 0.8597\n",
            "[CV] END epochs=3, first_layer_nodes=300, last_layer_nodes=100, n_layers=3; total time=  24.1s\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 8s 4ms/step - loss: 0.6603 - accuracy: 0.7972\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.4496 - accuracy: 0.8625\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3707 - accuracy: 0.8871\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4570 - accuracy: 0.8625\n",
            "[CV] END epochs=3, first_layer_nodes=300, last_layer_nodes=100, n_layers=3; total time=  25.2s\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6599 - accuracy: 0.7955\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4566 - accuracy: 0.8613\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3748 - accuracy: 0.8841\n",
            "782/782 [==============================] - 4s 4ms/step - loss: 0.4638 - accuracy: 0.8619\n",
            "[CV] END epochs=3, first_layer_nodes=300, last_layer_nodes=100, n_layers=3; total time=  22.7s\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6773 - accuracy: 0.7960\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4612 - accuracy: 0.8607\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3802 - accuracy: 0.8849\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.4897 - accuracy: 0.8559\n",
            "[CV] END epochs=3, first_layer_nodes=300, last_layer_nodes=50, n_layers=2; total time=  27.2s\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6851 - accuracy: 0.7935\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4650 - accuracy: 0.8604\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3808 - accuracy: 0.8860\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4801 - accuracy: 0.8574\n",
            "[CV] END epochs=3, first_layer_nodes=300, last_layer_nodes=50, n_layers=2; total time=  23.9s\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6710 - accuracy: 0.7961\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4601 - accuracy: 0.8612\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3732 - accuracy: 0.8862\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4809 - accuracy: 0.8578\n",
            "[CV] END epochs=3, first_layer_nodes=300, last_layer_nodes=50, n_layers=2; total time=  19.8s\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 3ms/step - loss: 0.6653 - accuracy: 0.7959\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4539 - accuracy: 0.8615\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3727 - accuracy: 0.8857\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4478 - accuracy: 0.8688\n",
            "[CV] END epochs=3, first_layer_nodes=300, last_layer_nodes=50, n_layers=3; total time=  22.0s\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 8s 4ms/step - loss: 0.6668 - accuracy: 0.7960\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4534 - accuracy: 0.8616\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3751 - accuracy: 0.8868\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4654 - accuracy: 0.8616\n",
            "[CV] END epochs=3, first_layer_nodes=300, last_layer_nodes=50, n_layers=3; total time=  25.3s\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6715 - accuracy: 0.7939\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4576 - accuracy: 0.8605\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3742 - accuracy: 0.8855\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4502 - accuracy: 0.8670\n",
            "[CV] END epochs=3, first_layer_nodes=300, last_layer_nodes=50, n_layers=3; total time=  24.7s\n",
            "Epoch 1/3\n",
            "2344/2344 [==============================] - 11s 4ms/step - loss: 0.6106 - accuracy: 0.8159\n",
            "Epoch 2/3\n",
            "2344/2344 [==============================] - 10s 4ms/step - loss: 0.4197 - accuracy: 0.8732\n",
            "Epoch 3/3\n",
            "2344/2344 [==============================] - 9s 4ms/step - loss: 0.3420 - accuracy: 0.8966\n",
            "Best: 0.869266668955485 using {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 2}\n",
            "Means: 0.862613320350647, Stdev: 0.005290528491438769 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 2}\n",
            "Means: 0.8667999903361002, Stdev: 0.0017272830220856042 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 3}\n",
            "Means: 0.869266668955485, Stdev: 0.0019613054331735266 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 2}\n",
            "Means: 0.8627466758092245, Stdev: 0.005363224788987473 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 3}\n",
            "Means: 0.8644266724586487, Stdev: 0.0018889896279805317 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 100, 'n_layers': 2}\n",
            "Means: 0.8613733251889547, Stdev: 0.0012221605066543783 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 100, 'n_layers': 3}\n",
            "Means: 0.8570533394813538, Stdev: 0.0008147463627322391 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 50, 'n_layers': 2}\n",
            "Means: 0.8658266464869181, Stdev: 0.0030463704537418332 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 50, 'n_layers': 3}\n",
            "CPU times: user 9min 11s, sys: 47.8 s, total: 9min 59s\n",
            "Wall time: 9min 48s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfH6okqe7mNo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b63f45fb-0572-4a3a-f9df-a4521d473ad1"
      },
      "source": [
        "\n",
        "\n",
        "best_model = grid_result.best_estimator_\n",
        "best_model.get_params()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epochs': 3,\n",
              " 'first_layer_nodes': 500,\n",
              " 'last_layer_nodes': 50,\n",
              " 'n_layers': 2,\n",
              " 'build_fn': <function __main__.create_model(n_layers, first_layer_nodes, last_layer_nodes, act_funct='relu', negative_node_incrementation=True)>}"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OJv_xJl4G2W7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layers = grid_result.cv_results_['param_n_layers']\n",
        "first_layer_nodes = grid_result.cv_results_['param_first_layer_nodes']\n",
        "last_layer_nodes = grid_result.cv_results_['param_last_layer_nodes']\n",
        "cv_accuracies = grid_result.cv_results_['mean_test_score']\n",
        "ranking = grid_result.cv_results_['rank_test_score']\n",
        "\n",
        "pd.DataFrame(data={\"Number of Layers\": layers, \n",
        "                   \"1st Layer Nodes\": first_layer_nodes, \n",
        "                   \"Last Layer Nodes\": last_layer_nodes, \n",
        "                   \"Mean CV Accuracy\": cv_accuracies, \n",
        "                   \"Rank\":ranking}).sort_values(by=[\"Rank\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "Zx6MZMbQEqHt",
        "outputId": "14bdb662-f1cf-4011-f6e7-3507207b1df8"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Number of Layers 1st Layer Nodes Last Layer Nodes  Mean CV Accuracy  Rank\n",
              "2                2             500               50          0.869267     1\n",
              "1                3             500              100          0.866800     2\n",
              "7                3             300               50          0.865827     3\n",
              "4                2             300              100          0.864427     4\n",
              "3                3             500               50          0.862747     5\n",
              "0                2             500              100          0.862613     6\n",
              "5                3             300              100          0.861373     7\n",
              "6                2             300               50          0.857053     8"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b9175960-3665-43d9-8e70-9812026cef18\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Number of Layers</th>\n",
              "      <th>1st Layer Nodes</th>\n",
              "      <th>Last Layer Nodes</th>\n",
              "      <th>Mean CV Accuracy</th>\n",
              "      <th>Rank</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>500</td>\n",
              "      <td>50</td>\n",
              "      <td>0.869267</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>500</td>\n",
              "      <td>100</td>\n",
              "      <td>0.866800</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3</td>\n",
              "      <td>300</td>\n",
              "      <td>50</td>\n",
              "      <td>0.865827</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>300</td>\n",
              "      <td>100</td>\n",
              "      <td>0.864427</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>500</td>\n",
              "      <td>50</td>\n",
              "      <td>0.862747</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>500</td>\n",
              "      <td>100</td>\n",
              "      <td>0.862613</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3</td>\n",
              "      <td>300</td>\n",
              "      <td>100</td>\n",
              "      <td>0.861373</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2</td>\n",
              "      <td>300</td>\n",
              "      <td>50</td>\n",
              "      <td>0.857053</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b9175960-3665-43d9-8e70-9812026cef18')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b9175960-3665-43d9-8e70-9812026cef18 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b9175960-3665-43d9-8e70-9812026cef18');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Inlda_0w7mNo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b2ef514-043f-4773-e9dd-5f5793c537aa"
      },
      "source": [
        "print\n",
        "\n",
        "grid_result.cv_results_.keys()"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time', 'param_epochs', 'param_first_layer_nodes', 'param_last_layer_nodes', 'param_n_layers', 'params', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'mean_test_score', 'std_test_score', 'rank_test_score'])"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrs3Yib17mNl"
      },
      "source": [
        "Ok, now that we've played around a bit with  `create_model`, let's build a  simpler model that we'll use to run gridsearches. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6azV65Nb7mNo"
      },
      "source": [
        "-----\n",
        "\n",
        "# Experiment 2: Run the Gridsearch Algorithms \n",
        "\n",
        "In this section, we are going to use the same model and dataset in order to benchmark 3 different gridsearch approaches: \n",
        "\n",
        "- Gridsearch\n",
        "- Random Search\n",
        "- Bayesian Optimization. \n",
        "\n",
        "\n",
        "Our goal in this experiment is two-fold. We want to see which appraoch \n",
        "\n",
        "- Scores the highest accuracy\n",
        "- Has the shortest run time \n",
        "\n",
        "We want to see how these 3 gridsearch approaches handle these trade-offs and to give you a sense of those trades offs.\n",
        "\n",
        "### Trade-offs\n",
        "\n",
        "`Gridsearch` will train a model on every single unique hyperparameter combination, this guarantees that you'll get the highest possible accuracy from your parameter set but your gridsearch might have a very long run-time. \n",
        "\n",
        "`Random Search` will randomly sample from your parameter set which, depending on how many samples, the run-time might be significantly cut down but you might or might not sample the parameters that correspond to the heightest possible accuracies. \n",
        "\n",
        "`Bayesian Optimization` has a bit of intelligence built into it's search algorithm but you do need to manually select some parameters which may greatly influence the model learning outcomes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X41u_hls7mNp"
      },
      "source": [
        "-------\n",
        "### Build our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ_uyKlj7mNp"
      },
      "source": [
        "# because gridsearching can take a lot of time and we are bench marking 3 different approaches\n",
        "# let's build a simple model to minimize run time \n",
        "\n",
        "def build_model(hp):\n",
        "    \n",
        "    \"\"\"\n",
        "    Returns a complied keras model ready for keras-tuner gridsearch algorithms \n",
        "    \"\"\"\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    # hidden layer\n",
        "    model.add(Dense(units=hp.get('units'),activation=hp.get(\"activation\")))\n",
        "    \n",
        "    # output layer\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=Adam(hp.get('learning_rate')),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "  "
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYE7rTku7mNp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "4b669535-1a87-4944-ceb8-723a8c154a98"
      },
      "source": [
        "# build out our hyperparameter dictionary \n",
        "hp = HyperParameters()\n",
        "hp.Int('units', min_value=32, max_value=512, step=32)\n",
        "hp.Choice('learning_rate',values=[1e-1, 1e-2, 1e-3])\n",
        "hp.Choice('activation',values=[\"relu\", \"sigmoid\"])"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'relu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqjp2kHD7mNu"
      },
      "source": [
        "---------\n",
        "## 2.1 Gridsearch Optimization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsNW4rJp7mNu"
      },
      "source": [
        "### Populate a `sklearn` compatible parameter dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJQFKoyL7mNu"
      },
      "source": [
        "# build out our hyperparameter dictionary \n",
        "hyper_parameters = {\n",
        "    # BUG Fix: cast array as list otherwise GridSearchCV will throw error\n",
        "    \"units\": np.arange(32, 512, 32).tolist(),\n",
        "    \"learning_rate\": [1e-1, 1e-2, 1e-3],\n",
        "    \"activation\":[\"relu\", \"sigmoid\"]\n",
        "}"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcxV58iC7mNu"
      },
      "source": [
        "hyper_parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOoMg9Ao7mNv"
      },
      "source": [
        "### Build a `sklearn` compatible model function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZFVl-I-7mNv"
      },
      "source": [
        "def build_model(units, learning_rate, activation):\n",
        "    \n",
        "    \"\"\"\n",
        "    Returns a complie keras model ready for keras-tuner gridsearch algorithms \n",
        "    \"\"\"\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    # hidden layer\n",
        "    model.add(Dense(units, activation=activation))\n",
        "    \n",
        "    # output layer\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqYmn3QFsqZ_"
      },
      "source": [
        "### Apply the \"wrapper\" to make the model compatible with `sklearn`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABSzrTrH7mNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf333f49-b656-4bb9-8992-1e25b94b1e66"
      },
      "source": [
        "model = KerasClassifier(build_fn = build_model)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-113-a93dc876c14b>:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(build_fn = build_model)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "tTawllrN7mNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20766b52-1591-49b8-e154-ae27455f5215"
      },
      "source": [
        "# save start time \n",
        "start = time()\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, \n",
        "                    param_grid=hyper_parameters, \n",
        "                    n_jobs=-2, \n",
        "                    verbose=1, \n",
        "                    cv=3)\n",
        "\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# save end time \n",
        "end = time()\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 90 candidates, totalling 270 fits\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.9851 - accuracy: 0.2430\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 2.0925 - accuracy: 0.2445\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.9858 - accuracy: 0.2596\n",
            "782/782 [==============================] - 4s 4ms/step - loss: 2.0114 - accuracy: 0.2506\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.8365 - accuracy: 0.3184\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.8683 - accuracy: 0.2999\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.9173 - accuracy: 0.3075\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8674 - accuracy: 0.2925\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.9398 - accuracy: 0.2953\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8221 - accuracy: 0.2950\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.9388 - accuracy: 0.2956\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.9918 - accuracy: 0.2649\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.8935 - accuracy: 0.3315\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0030 - accuracy: 0.2312\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.9022 - accuracy: 0.3291\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.7538 - accuracy: 0.3267\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.2113 - accuracy: 0.1803\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.1580 - accuracy: 0.1773\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.9282 - accuracy: 0.3127\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9612 - accuracy: 0.2608\n",
            "1563/1563 [==============================] - 7s 3ms/step - loss: 1.8657 - accuracy: 0.3709\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 1.9233 - accuracy: 0.2723\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.9226 - accuracy: 0.3425\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 2.0741 - accuracy: 0.2060\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.9521 - accuracy: 0.3146\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.7892 - accuracy: 0.3260\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9363 - accuracy: 0.3163\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.8248 - accuracy: 0.3254\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9370 - accuracy: 0.3108\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.8879 - accuracy: 0.3267\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.9766 - accuracy: 0.3122\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9613 - accuracy: 0.2657\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.9643 - accuracy: 0.3323\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9024 - accuracy: 0.3462\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.1761 - accuracy: 0.2365\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0740 - accuracy: 0.2334\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.0465 - accuracy: 0.2892\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9961 - accuracy: 0.2502\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.9584 - accuracy: 0.3283\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 1.8889 - accuracy: 0.2998\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0020 - accuracy: 0.3254\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9954 - accuracy: 0.2287\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.1121 - accuracy: 0.2831\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.9088 - accuracy: 0.2662\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0679 - accuracy: 0.3145\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 1.8765 - accuracy: 0.3103\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9849 - accuracy: 0.3285\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8353 - accuracy: 0.3049\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.0153 - accuracy: 0.3086\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 2.0123 - accuracy: 0.2300\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.1147 - accuracy: 0.3028\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 1.9239 - accuracy: 0.2730\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.9596 - accuracy: 0.3454\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8922 - accuracy: 0.2602\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.2363 - accuracy: 0.2556\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.1224 - accuracy: 0.2282\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0706 - accuracy: 0.3261\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 1.8972 - accuracy: 0.3013\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.9581 - accuracy: 0.3662\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9240 - accuracy: 0.3056\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.9479 - accuracy: 0.3404\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8782 - accuracy: 0.3265\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.0185 - accuracy: 0.3417\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.1335 - accuracy: 0.2319\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.1593 - accuracy: 0.2738\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 1.9947 - accuracy: 0.2416\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0592 - accuracy: 0.3386\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.9913 - accuracy: 0.3208\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.1435 - accuracy: 0.3387\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9311 - accuracy: 0.2924\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.0409 - accuracy: 0.3401\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8459 - accuracy: 0.3204\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.1349 - accuracy: 0.3220\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9925 - accuracy: 0.2454\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0785 - accuracy: 0.3431\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 1.8800 - accuracy: 0.2804\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.1433 - accuracy: 0.3201\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.8730 - accuracy: 0.3191\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.0839 - accuracy: 0.3318\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8333 - accuracy: 0.3338\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.1213 - accuracy: 0.3293\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8855 - accuracy: 0.3128\n",
            "1563/1563 [==============================] - 7s 3ms/step - loss: 2.0710 - accuracy: 0.3237\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.8762 - accuracy: 0.3043\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.1938 - accuracy: 0.3288\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.9355 - accuracy: 0.3112\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.3106 - accuracy: 0.3234\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 2.0827 - accuracy: 0.2078\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.1181 - accuracy: 0.3409\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.9193 - accuracy: 0.2792\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8013 - accuracy: 0.7566\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6864 - accuracy: 0.7958\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7817 - accuracy: 0.7638\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6936 - accuracy: 0.7913\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7871 - accuracy: 0.7567\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7306 - accuracy: 0.7860\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7551 - accuracy: 0.7707\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6781 - accuracy: 0.7957\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7574 - accuracy: 0.7723\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6518 - accuracy: 0.8078\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7471 - accuracy: 0.7737\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6771 - accuracy: 0.7974\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7509 - accuracy: 0.7705\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6677 - accuracy: 0.8036\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7468 - accuracy: 0.7755\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6480 - accuracy: 0.8048\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7356 - accuracy: 0.7757\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.7046 - accuracy: 0.7854\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7381 - accuracy: 0.7766\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6449 - accuracy: 0.8064\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7358 - accuracy: 0.7764\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6609 - accuracy: 0.8033\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7382 - accuracy: 0.7755\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6532 - accuracy: 0.7994\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7305 - accuracy: 0.7802\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6759 - accuracy: 0.8013\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7382 - accuracy: 0.7790\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.6603 - accuracy: 0.8026\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7329 - accuracy: 0.7789\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.6419 - accuracy: 0.8183\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7350 - accuracy: 0.7771\n",
            "782/782 [==============================] - 4s 4ms/step - loss: 0.6738 - accuracy: 0.8088\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7455 - accuracy: 0.7783\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6729 - accuracy: 0.7861\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7415 - accuracy: 0.7764\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6703 - accuracy: 0.7986\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7345 - accuracy: 0.7786\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6622 - accuracy: 0.8052\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7404 - accuracy: 0.7783\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6615 - accuracy: 0.7989\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7383 - accuracy: 0.7751\n",
            "782/782 [==============================] - 4s 4ms/step - loss: 0.6858 - accuracy: 0.8009\n",
            "1563/1563 [==============================] - 11s 5ms/step - loss: 0.7333 - accuracy: 0.7790\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6403 - accuracy: 0.8146\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7351 - accuracy: 0.7797\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6509 - accuracy: 0.8066\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7357 - accuracy: 0.7759\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6643 - accuracy: 0.8066\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7297 - accuracy: 0.7814\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6883 - accuracy: 0.8028\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7425 - accuracy: 0.7756\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6367 - accuracy: 0.8105\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7375 - accuracy: 0.7786\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6261 - accuracy: 0.8100\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7344 - accuracy: 0.7791\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6577 - accuracy: 0.8072\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7473 - accuracy: 0.7744\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6546 - accuracy: 0.8007\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7426 - accuracy: 0.7766\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.6660 - accuracy: 0.8039\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7380 - accuracy: 0.7773\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6613 - accuracy: 0.8053\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7405 - accuracy: 0.7783\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6773 - accuracy: 0.8008\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7428 - accuracy: 0.7768\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6686 - accuracy: 0.8048\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7419 - accuracy: 0.7770\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6711 - accuracy: 0.8016\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7385 - accuracy: 0.7809\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6513 - accuracy: 0.8022\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7465 - accuracy: 0.7752\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7104 - accuracy: 0.7879\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7377 - accuracy: 0.7789\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6994 - accuracy: 0.7905\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7433 - accuracy: 0.7800\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6526 - accuracy: 0.8008\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7387 - accuracy: 0.7765\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6576 - accuracy: 0.8126\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7423 - accuracy: 0.7765\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6464 - accuracy: 0.8116\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7393 - accuracy: 0.7779\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6886 - accuracy: 0.7989\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7348 - accuracy: 0.7785\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6699 - accuracy: 0.8076\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7353 - accuracy: 0.7780\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6641 - accuracy: 0.8038\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7409 - accuracy: 0.7794\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6462 - accuracy: 0.8092\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7444 - accuracy: 0.7769\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6877 - accuracy: 0.7982\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8526 - accuracy: 0.7481\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7223 - accuracy: 0.7921\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8690 - accuracy: 0.7423\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.7115 - accuracy: 0.7910\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8523 - accuracy: 0.7505\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.7159 - accuracy: 0.7936\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7988 - accuracy: 0.7625\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6453 - accuracy: 0.8118\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8063 - accuracy: 0.7629\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6374 - accuracy: 0.8130\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8029 - accuracy: 0.7605\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6515 - accuracy: 0.8123\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7712 - accuracy: 0.7708\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6163 - accuracy: 0.8199\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7776 - accuracy: 0.7716\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6220 - accuracy: 0.8148\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7576 - accuracy: 0.7755\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6199 - accuracy: 0.8195\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7349 - accuracy: 0.7810\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5891 - accuracy: 0.8294\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7424 - accuracy: 0.7795\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6034 - accuracy: 0.8215\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7372 - accuracy: 0.7818\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5931 - accuracy: 0.8273\n",
            "1563/1563 [==============================] - 7s 3ms/step - loss: 0.7140 - accuracy: 0.7875\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5998 - accuracy: 0.8216\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7272 - accuracy: 0.7861\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5904 - accuracy: 0.8217\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7238 - accuracy: 0.7828\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5783 - accuracy: 0.8330\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7126 - accuracy: 0.7859\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5656 - accuracy: 0.8343\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7119 - accuracy: 0.7896\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5550 - accuracy: 0.8355\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7157 - accuracy: 0.7874\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5725 - accuracy: 0.8340\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6962 - accuracy: 0.7935\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5700 - accuracy: 0.8294\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6959 - accuracy: 0.7935\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5464 - accuracy: 0.8362\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7017 - accuracy: 0.7901\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5706 - accuracy: 0.8330\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6973 - accuracy: 0.7920\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5598 - accuracy: 0.8348\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6912 - accuracy: 0.7946\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5436 - accuracy: 0.8375\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6927 - accuracy: 0.7934\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5629 - accuracy: 0.8364\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6850 - accuracy: 0.7956\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5380 - accuracy: 0.8439\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6879 - accuracy: 0.7955\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5449 - accuracy: 0.8379\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6775 - accuracy: 0.7987\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5492 - accuracy: 0.8411\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6883 - accuracy: 0.7953\n",
            "782/782 [==============================] - 3s 2ms/step - loss: 0.5393 - accuracy: 0.8380\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6806 - accuracy: 0.7981\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.5279 - accuracy: 0.8407\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6807 - accuracy: 0.7956\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5457 - accuracy: 0.8397\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6740 - accuracy: 0.7982\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.5598 - accuracy: 0.8337\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6781 - accuracy: 0.7992\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5554 - accuracy: 0.8329\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6727 - accuracy: 0.7986\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5340 - accuracy: 0.8435\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6688 - accuracy: 0.7998\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5522 - accuracy: 0.8354\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6748 - accuracy: 0.7997\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5488 - accuracy: 0.8346\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6694 - accuracy: 0.8009\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.5294 - accuracy: 0.8468\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6676 - accuracy: 0.8012\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5363 - accuracy: 0.8395\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6661 - accuracy: 0.8003\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5175 - accuracy: 0.8461\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6656 - accuracy: 0.8024\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5317 - accuracy: 0.8427\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6651 - accuracy: 0.8015\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.5288 - accuracy: 0.8448\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6655 - accuracy: 0.8025\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5261 - accuracy: 0.8445\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6675 - accuracy: 0.7998\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5545 - accuracy: 0.8371\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6620 - accuracy: 0.8014\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5340 - accuracy: 0.8421\n",
            "1563/1563 [==============================] - 7s 3ms/step - loss: 0.6652 - accuracy: 0.8000\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5317 - accuracy: 0.8413\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6624 - accuracy: 0.8031\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5376 - accuracy: 0.8434\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2158 - accuracy: 0.6095\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.0894 - accuracy: 0.6586\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.1399 - accuracy: 0.6372\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.1225 - accuracy: 0.6446\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.1814 - accuracy: 0.6174\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.0827 - accuracy: 0.6649\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.1465 - accuracy: 0.6418\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.0738 - accuracy: 0.6578\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1975 - accuracy: 0.6261\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.0349 - accuracy: 0.6596\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.1902 - accuracy: 0.6176\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.0192 - accuracy: 0.6920\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.2362 - accuracy: 0.6117\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.2803 - accuracy: 0.5802\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1318 - accuracy: 0.6435\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.0148 - accuracy: 0.6904\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.2956 - accuracy: 0.6053\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.3209 - accuracy: 0.6436\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.1959 - accuracy: 0.6287\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.0403 - accuracy: 0.6889\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2052 - accuracy: 0.6346\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.0671 - accuracy: 0.6817\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.2099 - accuracy: 0.6341\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.1670 - accuracy: 0.6701\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3984 - accuracy: 0.6036\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.2600 - accuracy: 0.6514\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.1788 - accuracy: 0.6430\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.0831 - accuracy: 0.6639\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3758 - accuracy: 0.6064\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 1.5834 - accuracy: 0.6116\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.2248 - accuracy: 0.6371\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.1104 - accuracy: 0.6889\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4664 - accuracy: 0.5958\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.2655 - accuracy: 0.6321\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.5023 - accuracy: 0.5786\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.4698 - accuracy: 0.6116\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2847 - accuracy: 0.6194\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.2798 - accuracy: 0.6601\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2274 - accuracy: 0.6299\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.1108 - accuracy: 0.6639\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1708 - accuracy: 0.6423\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.0508 - accuracy: 0.6958\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.4968 - accuracy: 0.5785\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 2.2333 - accuracy: 0.5466\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.3787 - accuracy: 0.6193\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 1.1989 - accuracy: 0.6506\n",
            "1563/1563 [==============================] - 13s 6ms/step - loss: 1.3995 - accuracy: 0.6179\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 1.0853 - accuracy: 0.6906\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4899 - accuracy: 0.5930\n",
            "782/782 [==============================] - 4s 4ms/step - loss: 1.3447 - accuracy: 0.6430\n",
            "1563/1563 [==============================] - 8s 4ms/step - loss: 1.3914 - accuracy: 0.5979\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.2458 - accuracy: 0.6475\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2123 - accuracy: 0.6435\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.1840 - accuracy: 0.6652\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.3055 - accuracy: 0.6256\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.1472 - accuracy: 0.6817\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.4239 - accuracy: 0.6031\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.5279 - accuracy: 0.5337\n",
            "1563/1563 [==============================] - 7s 3ms/step - loss: 1.3435 - accuracy: 0.6131\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 1.3453 - accuracy: 0.6244\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.4343 - accuracy: 0.5949\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 1.2887 - accuracy: 0.6250\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4471 - accuracy: 0.5894\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.3378 - accuracy: 0.5961\n",
            "1563/1563 [==============================] - 7s 3ms/step - loss: 1.1908 - accuracy: 0.6357\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 1.1170 - accuracy: 0.6514\n",
            "1563/1563 [==============================] - 7s 3ms/step - loss: 1.7463 - accuracy: 0.5936\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 2.1662 - accuracy: 0.5913\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.3331 - accuracy: 0.6155\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.3594 - accuracy: 0.6060\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.3102 - accuracy: 0.6284\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 1.2470 - accuracy: 0.6458\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.5695 - accuracy: 0.5902\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.3570 - accuracy: 0.6392\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.9078 - accuracy: 0.5757\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.5565 - accuracy: 0.5838\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6029 - accuracy: 0.5823\n",
            "782/782 [==============================] - 4s 4ms/step - loss: 1.4178 - accuracy: 0.6528\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8267 - accuracy: 0.5779\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 1.6977 - accuracy: 0.6034\n",
            "1563/1563 [==============================] - 8s 4ms/step - loss: 1.6947 - accuracy: 0.5896\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 1.5920 - accuracy: 0.6290\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.6350 - accuracy: 0.5905\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.5261 - accuracy: 0.5993\n",
            "1563/1563 [==============================] - 7s 3ms/step - loss: 1.9120 - accuracy: 0.5809\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 1.6925 - accuracy: 0.6256\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.1502 - accuracy: 0.5641\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 2.0558 - accuracy: 0.5659\n",
            "1563/1563 [==============================] - 8s 4ms/step - loss: 1.8150 - accuracy: 0.5905\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.4644 - accuracy: 0.6371\n",
            "1563/1563 [==============================] - 8s 4ms/step - loss: 0.7826 - accuracy: 0.7609\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6786 - accuracy: 0.7956\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7871 - accuracy: 0.7617\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6961 - accuracy: 0.7854\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7884 - accuracy: 0.7610\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.7055 - accuracy: 0.7863\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7386 - accuracy: 0.7723\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6627 - accuracy: 0.8009\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7353 - accuracy: 0.7757\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6622 - accuracy: 0.7990\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7268 - accuracy: 0.7763\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6414 - accuracy: 0.8053\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7125 - accuracy: 0.7829\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6156 - accuracy: 0.8132\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7159 - accuracy: 0.7806\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6213 - accuracy: 0.8066\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7006 - accuracy: 0.7841\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6182 - accuracy: 0.8157\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6938 - accuracy: 0.7858\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6291 - accuracy: 0.8097\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6902 - accuracy: 0.7885\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5792 - accuracy: 0.8234\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6937 - accuracy: 0.7877\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6062 - accuracy: 0.8168\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6905 - accuracy: 0.7864\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6005 - accuracy: 0.8228\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6963 - accuracy: 0.7847\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5911 - accuracy: 0.8170\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7001 - accuracy: 0.7850\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6045 - accuracy: 0.8141\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6840 - accuracy: 0.7889\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.5922 - accuracy: 0.8180\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6857 - accuracy: 0.7900\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5825 - accuracy: 0.8233\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6833 - accuracy: 0.7894\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.5869 - accuracy: 0.8220\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6879 - accuracy: 0.7879\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6330 - accuracy: 0.8063\n",
            "1563/1563 [==============================] - 7s 3ms/step - loss: 0.6846 - accuracy: 0.7903\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6058 - accuracy: 0.8203\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6841 - accuracy: 0.7891\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5972 - accuracy: 0.8168\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6808 - accuracy: 0.7897\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5899 - accuracy: 0.8201\n",
            "1563/1563 [==============================] - 8s 4ms/step - loss: 0.6829 - accuracy: 0.7911\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5878 - accuracy: 0.8219\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6845 - accuracy: 0.7901\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6034 - accuracy: 0.8142\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6855 - accuracy: 0.7900\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6235 - accuracy: 0.8103\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6795 - accuracy: 0.7928\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6213 - accuracy: 0.8173\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6807 - accuracy: 0.7906\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5831 - accuracy: 0.8247\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6823 - accuracy: 0.7905\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5729 - accuracy: 0.8290\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6905 - accuracy: 0.7898\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5901 - accuracy: 0.8197\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6850 - accuracy: 0.7885\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5868 - accuracy: 0.8250\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6825 - accuracy: 0.7895\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6220 - accuracy: 0.8154\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6849 - accuracy: 0.7915\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6405 - accuracy: 0.8065\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6855 - accuracy: 0.7890\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6535 - accuracy: 0.8100\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6810 - accuracy: 0.7895\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6092 - accuracy: 0.8170\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6878 - accuracy: 0.7893\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6228 - accuracy: 0.8182\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6762 - accuracy: 0.7920\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6070 - accuracy: 0.8128\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6801 - accuracy: 0.7915\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5910 - accuracy: 0.8229\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6854 - accuracy: 0.7903\n",
            "782/782 [==============================] - 4s 4ms/step - loss: 0.5994 - accuracy: 0.8138\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6817 - accuracy: 0.7903\n",
            "782/782 [==============================] - 4s 4ms/step - loss: 0.5918 - accuracy: 0.8212\n",
            "1563/1563 [==============================] - 8s 4ms/step - loss: 0.6868 - accuracy: 0.7899\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.5999 - accuracy: 0.8232\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6879 - accuracy: 0.7889\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6400 - accuracy: 0.8050\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6899 - accuracy: 0.7884\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6114 - accuracy: 0.8122\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6898 - accuracy: 0.7888\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6060 - accuracy: 0.8191\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6942 - accuracy: 0.7889\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5915 - accuracy: 0.8219\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6853 - accuracy: 0.7886\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6401 - accuracy: 0.8034\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0488 - accuracy: 0.7018\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.8106 - accuracy: 0.7653\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.0447 - accuracy: 0.7014\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.8040 - accuracy: 0.7603\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.0365 - accuracy: 0.7082\n",
            "782/782 [==============================] - 4s 4ms/step - loss: 0.8229 - accuracy: 0.7584\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.9436 - accuracy: 0.7240\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.7521 - accuracy: 0.7781\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.9432 - accuracy: 0.7274\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.7505 - accuracy: 0.7778\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.9352 - accuracy: 0.7301\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.7611 - accuracy: 0.7764\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.8891 - accuracy: 0.7385\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.7249 - accuracy: 0.7904\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8962 - accuracy: 0.7393\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.7235 - accuracy: 0.7846\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8893 - accuracy: 0.7395\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.7444 - accuracy: 0.7782\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8733 - accuracy: 0.7431\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.7196 - accuracy: 0.7906\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8707 - accuracy: 0.7447\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6937 - accuracy: 0.7953\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8812 - accuracy: 0.7413\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.7365 - accuracy: 0.7773\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8561 - accuracy: 0.7480\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6970 - accuracy: 0.7954\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8537 - accuracy: 0.7485\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6911 - accuracy: 0.7900\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8519 - accuracy: 0.7491\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.7035 - accuracy: 0.7948\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8483 - accuracy: 0.7482\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6924 - accuracy: 0.7953\n",
            "1563/1563 [==============================] - 7s 3ms/step - loss: 0.8507 - accuracy: 0.7496\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6840 - accuracy: 0.7998\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8414 - accuracy: 0.7535\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6969 - accuracy: 0.7975\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8348 - accuracy: 0.7528\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6781 - accuracy: 0.8057\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8393 - accuracy: 0.7526\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6747 - accuracy: 0.8039\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8356 - accuracy: 0.7544\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.6943 - accuracy: 0.7962\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8318 - accuracy: 0.7517\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6976 - accuracy: 0.7957\n",
            "1563/1563 [==============================] - 7s 3ms/step - loss: 0.8319 - accuracy: 0.7548\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6726 - accuracy: 0.8014\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8274 - accuracy: 0.7547\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6944 - accuracy: 0.7910\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8216 - accuracy: 0.7555\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6752 - accuracy: 0.8052\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8304 - accuracy: 0.7550\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.6676 - accuracy: 0.8029\n",
            "1563/1563 [==============================] - 8s 4ms/step - loss: 0.8191 - accuracy: 0.7562\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6872 - accuracy: 0.7980\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8225 - accuracy: 0.7559\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6745 - accuracy: 0.8036\n",
            "1563/1563 [==============================] - 8s 4ms/step - loss: 0.8285 - accuracy: 0.7539\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6673 - accuracy: 0.8007\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8199 - accuracy: 0.7585\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.6943 - accuracy: 0.7954\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8181 - accuracy: 0.7571\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6747 - accuracy: 0.8003\n",
            "1563/1563 [==============================] - 7s 3ms/step - loss: 0.8230 - accuracy: 0.7557\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.6516 - accuracy: 0.8077\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8229 - accuracy: 0.7528\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6867 - accuracy: 0.7977\n",
            "1563/1563 [==============================] - 8s 4ms/step - loss: 0.8153 - accuracy: 0.7570\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6834 - accuracy: 0.7970\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8196 - accuracy: 0.7557\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.6590 - accuracy: 0.8039\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8094 - accuracy: 0.7588\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.6724 - accuracy: 0.8017\n",
            "1563/1563 [==============================] - 7s 3ms/step - loss: 0.8142 - accuracy: 0.7574\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6622 - accuracy: 0.8056\n",
            "1563/1563 [==============================] - 8s 4ms/step - loss: 0.8178 - accuracy: 0.7569\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6560 - accuracy: 0.8048\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8077 - accuracy: 0.7584\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6748 - accuracy: 0.8030\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.8125 - accuracy: 0.7563\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.6637 - accuracy: 0.8028\n",
            "1563/1563 [==============================] - 9s 4ms/step - loss: 0.8144 - accuracy: 0.7567\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6582 - accuracy: 0.8054\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8112 - accuracy: 0.7577\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6622 - accuracy: 0.8064\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8128 - accuracy: 0.7573\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6643 - accuracy: 0.8094\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.8137 - accuracy: 0.7586\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6632 - accuracy: 0.8026\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8079 - accuracy: 0.7585\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6639 - accuracy: 0.8054\n",
            "2344/2344 [==============================] - 10s 4ms/step - loss: 0.6177 - accuracy: 0.8157\n",
            "Best: 0.8427866498629252 using {'activation': 'relu', 'learning_rate': 0.001, 'units': 416}\n",
            "Means: 0.26499999066193897, Stdev: 0.024814985818366556 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 32}\n",
            "Means: 0.28411999344825745, Stdev: 0.013614147526136825 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 64}\n",
            "Means: 0.24505333105723062, Stdev: 0.06177392651802086 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 96}\n",
            "Means: 0.24634667237599692, Stdev: 0.028939704736259234 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 128}\n",
            "Means: 0.32603999972343445, Stdev: 0.0005256139076642272 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 160}\n",
            "Means: 0.2817866653203964, Stdev: 0.047415236936773096 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 192}\n",
            "Means: 0.2595600038766861, Stdev: 0.02975722861404883 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 224}\n",
            "Means: 0.2937999963760376, Stdev: 0.01964209751710772 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 256}\n",
            "Means: 0.25439999997615814, Stdev: 0.018005067019084987 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 288}\n",
            "Means: 0.27833332618077594, Stdev: 0.03552090517878005 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 320}\n",
            "Means: 0.26663999756177265, Stdev: 0.04249741279764411 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 352}\n",
            "Means: 0.3112000028292338, Stdev: 0.013294610718012054 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 384}\n",
            "Means: 0.2816399981578191, Stdev: 0.030124341735197332 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 416}\n",
            "Means: 0.3169866700967153, Stdev: 0.012409778138772781 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 448}\n",
            "Means: 0.26609333356221515, Stdev: 0.043240290480162716 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 480}\n",
            "Means: 0.7910400032997131, Stdev: 0.004005712600036703 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 32}\n",
            "Means: 0.8003066579500834, Stdev: 0.005342795046935507 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 64}\n",
            "Means: 0.7979199886322021, Stdev: 0.008893029910826607 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 96}\n",
            "Means: 0.8030133247375488, Stdev: 0.0028659651991841326 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 128}\n",
            "Means: 0.8074000080426534, Stdev: 0.007713311788221261 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 160}\n",
            "Means: 0.7978399991989136, Stdev: 0.009274648114802799 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 192}\n",
            "Means: 0.801693320274353, Stdev: 0.002637436390363845 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 224}\n",
            "Means: 0.8092399835586548, Stdev: 0.0037618523981756405 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 256}\n",
            "Means: 0.8077866633733114, Stdev: 0.003502432540327826 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 288}\n",
            "Means: 0.8039199908574423, Stdev: 0.002629288976484838 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 320}\n",
            "Means: 0.8036266764005026, Stdev: 0.0020117785348258894 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 352}\n",
            "Means: 0.7972533305486044, Stdev: 0.006632468501184439 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 384}\n",
            "Means: 0.8012800017992655, Stdev: 0.00900530690160229 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 416}\n",
            "Means: 0.8060266772905985, Stdev: 0.005320905026035149 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 448}\n",
            "Means: 0.8037333289782206, Stdev: 0.004490969397249876 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 480}\n",
            "Means: 0.7922266721725464, Stdev: 0.0010804163457607497 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 32}\n",
            "Means: 0.8123733401298523, Stdev: 0.0004781556477636973 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 64}\n",
            "Means: 0.8180933396021525, Stdev: 0.002306234128507606 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 96}\n",
            "Means: 0.8260799845059713, Stdev: 0.003365875473458155 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 128}\n",
            "Means: 0.8254000147183737, Stdev: 0.005345948043267658 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 160}\n",
            "Means: 0.8346133430798849, Stdev: 0.0006485691736481117 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 192}\n",
            "Means: 0.8328800002733866, Stdev: 0.0027783911116132176 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 224}\n",
            "Means: 0.8362266620000204, Stdev: 0.0011144288762992814 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 256}\n",
            "Means: 0.8409733374913534, Stdev: 0.002450643633713976 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 288}\n",
            "Means: 0.8394400080045065, Stdev: 0.0011233285842472969 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 320}\n",
            "Means: 0.8367066780726115, Stdev: 0.004800578507590173 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 352}\n",
            "Means: 0.8389333287874857, Stdev: 0.005572177184917135 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 384}\n",
            "Means: 0.8427866498629252, Stdev: 0.0026948603034347364 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 416}\n",
            "Means: 0.842146654923757, Stdev: 0.0035562221517864245 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 448}\n",
            "Means: 0.8422533273696899, Stdev: 0.0008418082482464257 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 480}\n",
            "Means: 0.6560399929682413, Stdev: 0.008480949981312432 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 32}\n",
            "Means: 0.669813334941864, Stdev: 0.01567727275221451 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 64}\n",
            "Means: 0.6380933324495951, Stdev: 0.04517585955651672 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 96}\n",
            "Means: 0.6802266836166382, Stdev: 0.007759754663943028 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 128}\n",
            "Means: 0.6422799825668335, Stdev: 0.02231930487424291 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 160}\n",
            "Means: 0.6442000071207682, Stdev: 0.032716849928509255 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 192}\n",
            "Means: 0.6732933322588602, Stdev: 0.01601820130207832 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 224}\n",
            "Means: 0.6292666594187418, Stdev: 0.06069230393108523 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 256}\n",
            "Means: 0.6519333322842916, Stdev: 0.009585334613825146 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 288}\n",
            "Means: 0.6132800181706747, Stdev: 0.060930238610764645 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 320}\n",
            "Means: 0.6241333285967509, Stdev: 0.02257552245584255 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 352}\n",
            "Means: 0.6143466631571451, Stdev: 0.023034175235717415 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 384}\n",
            "Means: 0.6252533396085104, Stdev: 0.029867009273503914 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 416}\n",
            "Means: 0.6105733315149943, Stdev: 0.013135661450904423 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 448}\n",
            "Means: 0.6095466613769531, Stdev: 0.031200246959278033 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 480}\n",
            "Means: 0.7890666524569193, Stdev: 0.004606827783445689 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 32}\n",
            "Means: 0.801746686299642, Stdev: 0.0026360266971837886 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 64}\n",
            "Means: 0.8118000030517578, Stdev: 0.0038454313211866383 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 96}\n",
            "Means: 0.8166399995485941, Stdev: 0.005602965996042435 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 128}\n",
            "Means: 0.8179733157157898, Stdev: 0.003641871413140748 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 160}\n",
            "Means: 0.821120003859202, Stdev: 0.0022359380859532253 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 192}\n",
            "Means: 0.814466675122579, Stdev: 0.005956759090702126 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 224}\n",
            "Means: 0.8187199831008911, Stdev: 0.003303489311329922 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 256}\n",
            "Means: 0.8174400130907694, Stdev: 0.005879866377004203 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 288}\n",
            "Means: 0.8245733181635538, Stdev: 0.0038331020900088243 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 320}\n",
            "Means: 0.8106533288955688, Stdev: 0.0036511018530773352 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 352}\n",
            "Means: 0.8159866531689962, Stdev: 0.002333595532486244 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 384}\n",
            "Means: 0.81932000319163, Stdev: 0.003932384697616693 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 416}\n",
            "Means: 0.8134666681289673, Stdev: 0.007483899955006391 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 448}\n",
            "Means: 0.8148266474405924, Stdev: 0.008132331299078173 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 480}\n",
            "Means: 0.761346677939097, Stdev: 0.0029168555728569367 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 32}\n",
            "Means: 0.7774400115013123, Stdev: 0.0007418750434165129 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 64}\n",
            "Means: 0.7844400207201639, Stdev: 0.004982635529995135 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 96}\n",
            "Means: 0.7877199848492941, Stdev: 0.007625436964988409 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 128}\n",
            "Means: 0.7934000094731649, Stdev: 0.00244642896136972 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 160}\n",
            "Means: 0.7975466450055441, Stdev: 0.0018458837548127461 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 192}\n",
            "Means: 0.8019066452980042, Stdev: 0.004129414730419169 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 224}\n",
            "Means: 0.7960533301035563, Stdev: 0.00423601653786071 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 256}\n",
            "Means: 0.8020533124605814, Stdev: 0.002986611007476305 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 288}\n",
            "Means: 0.7998800079027811, Stdev: 0.0033789888517030795 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 320}\n",
            "Means: 0.8018933534622192, Stdev: 0.004231348414179997 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 352}\n",
            "Means: 0.8008933266003927, Stdev: 0.002868927783351487 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 384}\n",
            "Means: 0.8044666647911072, Stdev: 0.0010654807007844452 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 416}\n",
            "Means: 0.8048800031344095, Stdev: 0.0014991598083885652 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 448}\n",
            "Means: 0.8057866891225179, Stdev: 0.0027599706686943725 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 480}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THKMZLNv7mNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36ad4b34-915d-4a26-b81c-7b7f46b1d1bc"
      },
      "source": [
        "# total run time \n",
        "total_run_time_in_miniutes = (end - start)/60\n",
        "total_run_time_in_miniutes"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "52.86374278863271"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XgJsrZb7mNx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42e558f3-8473-4c70-c6ea-3257423b2e07"
      },
      "source": [
        "# Best parameter combination\n",
        "grid_result.best_params_"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': 'relu', 'learning_rate': 0.001, 'units': 416}"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYufbSI87mNx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5cc246d-18e1-4810-a9cd-05694920b172"
      },
      "source": [
        "# because all other optimization approaches are reporting test set score\n",
        "# let's calculate the test set score in this case \n",
        "best_model = grid_result.best_estimator_\n",
        "\n",
        "test_acc = best_model.score(X_test, y_test)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 3s 3ms/step - loss: 0.4933 - accuracy: 0.8539\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlR-pVwP7mNx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23580e2c-1f1f-45cd-dc16-4b858bee7116"
      },
      "source": [
        "test_acc, grid_result.best_score_"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8539199829101562, 0.8427866498629252)"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj4jJ0Qm7mNx"
      },
      "source": [
        " ### Results\n",
        " \n",
        "Identify and write the the best performing hyperparameter combination and model score. \n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "9577db883482c6cded3836e5cfbf5a74",
          "grade": true,
          "grade_id": "cell-eb06d682d2790f6e",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "10px3N2q7mNx"
      },
      "source": [
        "The grid search spanned a total of ~53 minutes!\n",
        "\n",
        "Best hyperparameters:\n",
        "\n",
        "'activation': 'relu'\n",
        "\n",
        "'learning_rate': 0.001\n",
        "\n",
        "'units': 416\n",
        "\n",
        "This model resulted in validation accuracy of 0.8428 and testing accuracy of 0.8539. These are relatively lower than those of models found by keras-tuner algorithms; each model in the grid-search was only trained for 1 epoch. Validation and testing accuracy are expected to be higher.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu-lBWph7mNq"
      },
      "source": [
        "------\n",
        "## 2.2 Random Search with `keras-tuner`\n",
        "\n",
        "Be sure to check out the [**docs for Keras-Tuner**](https://keras-team.github.io/keras-tuner/documentation/tuners/). Here you can read about the input parameters for the `RandomSearch` tuner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "aaff9aae33845f374e15f2381719d83a",
          "grade": false,
          "grade_id": "cell-8c1dfb9b6d12bea2",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "8DApqLli7mNq"
      },
      "source": [
        "# how many unique hyperparameter combinations do we have? \n",
        "# HINT: take the product of the number of possible values for each hyperparameter \n",
        "\n",
        "n_unique_hparam_combos = 90 # does not include 512 units"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a9d628451e83431e1b52da10eccf2c00",
          "grade": false,
          "grade_id": "cell-1fa83950bb2d5f92",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "m1UKRA597mNq"
      },
      "source": [
        "# how many of these do we want to randomly sample?\n",
        "# let's pick 25% of n_unique_hparam_combos param combos to sample\n",
        "# 90 unique combos -> ! 23 for random search\n",
        "\n",
        "fraction_to_sample = 0.25\n",
        "n_param_combos_to_sample = np.round(n_unique_hparam_combos * fraction_to_sample)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TzaNnzoQU4U"
      },
      "source": [
        "### Instantiate a `RandomSearch()` object for your grid search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9PCHLBWQPcb"
      },
      "source": [
        "random_tuner = RandomSearch(\n",
        "            build_model,\n",
        "            objective='val_accuracy', # this does not use cross-validation \n",
        "            max_trials=n_param_combos_to_sample, # number of times to sample the parameter set and build a model \n",
        "            seed=1234,\n",
        "            hyperparameters=hp, # pass in our hyperparameter dictionary\n",
        "            directory='./keras-tuner-trial',\n",
        "            project_name='random_search')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGFdv1qE7mNr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4fb1482-c4bc-40cd-c426-d8248e67341f"
      },
      "source": [
        " # take note of Total elapsed time in print out -- took ~10 minutes without GPU\n",
        "random_tuner.search(X_train, y_train,\n",
        "                    epochs=3,\n",
        "                    validation_data=(X_test, y_test)) # shouldn't validate on testing data; choosing the best RandomSearch model here 'leaks' information into our model\n",
        "                                                      # need to partition from training data for validation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 20 Complete [00h 00m 34s]\n",
            "val_accuracy: 0.8131999969482422\n",
            "\n",
            "Best val_accuracy So Far: 0.8730000257492065\n",
            "Total elapsed time: 00h 10m 09s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNBUhIe97mNr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a12fb52d-eec7-4b72-a2d0-cfb8482ade48"
      },
      "source": [
        "# identify the best score and hyperparamter (should be at the top since scores are ranked)\n",
        "random_tuner.results_summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results summary\n",
            "Results in ./keras-tuner-trial/random_search\n",
            "Showing 10 best trials\n",
            "Objective(name='val_accuracy', direction='max')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 384\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8730000257492065\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 352\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8729199767112732\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 448\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8636400103569031\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 480\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8621199727058411\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 416\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8591200113296509\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 288\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8572400212287903\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 224\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8525599837303162\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 320\n",
            "learning_rate: 0.01\n",
            "activation: sigmoid\n",
            "Score: 0.8378400206565857\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 160\n",
            "learning_rate: 0.01\n",
            "activation: sigmoid\n",
            "Score: 0.8370400071144104\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 192\n",
            "learning_rate: 0.01\n",
            "activation: sigmoid\n",
            "Score: 0.8350800275802612\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "FRpQVXBE7mNr"
      },
      "source": [
        " ### Results\n",
        " \n",
        "Identify and write the the best performing hyperparameter combination and model score. \n",
        "Note that because this is Random Search, multiple runs might have slighly different outcomes. \n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "f084b5d373f8589a1de8d6d4473b974a",
          "grade": true,
          "grade_id": "cell-5527738b6382c164",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "aQjMc84c7mNs"
      },
      "source": [
        "From the random_tuner.results_summary():\n",
        "\n",
        "Units: 384\n",
        "\n",
        "Learning_rate: 0.001\n",
        "\n",
        "A\n",
        "ctivation: relu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXjW7eYA7mNs"
      },
      "source": [
        "------\n",
        "## 2.3 Bayesian Optimization with `keras-tuner`\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/0/02/GpParBayesAnimationSmall.gif)\n",
        "\n",
        "Be sure to check out the [**docs for Keras-Tuner**](https://keras-team.github.io/keras-tuner/documentation/tuners/). Here you can read about the input parameters for the `BayesianOptimization` tuner.\n",
        "\n",
        "Pay special attention to these `BayesianOptimization` parameters: `num_initial_points` and `beta`. \n",
        "\n",
        "`num_initial_points`: \n",
        "\n",
        "Number of randomly selected hyperparameter combinations to try before applying bayesian probability to determine likelihood of which param combo to try next based on expected improvement\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "`beta`: \n",
        "\n",
        "Larger values means more willing to explore new hyperparameter combinations (analogous to searching for the global minimum in Gradient Descent), smaller values means that it is less willing to try new hyperparameter combinations (analogous to getting stuck in a local minimum in Gradient Descent). \n",
        "\n",
        "As a start, error on the side of larger values. What defines a small or large value you ask? That question would pull us into the mathematical intricacies of Bayesian Optimization and Gaussian Processes. For simplicity, notice that the default value is 2.6 and work from there. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NXjQBn47mNs"
      },
      "source": [
        "# we know that 23 samples is about 25% of 90 possible hyper-parameter combos\n",
        "# let's set up a run with the same parameters we used for RandomSearch() so the comparison will be apples-to-apples\n",
        "# feel free to play with any of these numbers later\n",
        "max_trials=23\n",
        "num_initial_points=5\n",
        "beta=5.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhZNIJZ4RS5Y"
      },
      "source": [
        "#### Instantiate a `BayesianOptimization()` object for your grid search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33joO_J97mNs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9241d1ca-eed3-433f-9188-e854e808afd1"
      },
      "source": [
        "bayesian_tuner = BayesianOptimization(\n",
        "                    build_model,\n",
        "                    objective='val_accuracy',\n",
        "                    max_trials=max_trials,\n",
        "                    hyperparameters=hp, # pass in our hyperparameter dictionary\n",
        "                    num_initial_points=num_initial_points, \n",
        "                    beta=beta, \n",
        "                    seed=1234,\n",
        "                    directory='./keras-tuner-trial',\n",
        "                    project_name='bayesian_optimization_4')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Reloading Oracle from existing project ./keras-tuner-trial/bayesian_optimization_4/oracle.json\n",
            "INFO:tensorflow:Reloading Tuner from ./keras-tuner-trial/bayesian_optimization_4/tuner0.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9AM5Pdj7mNt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ca1e102-3836-4d46-c535-572f8a7f085c"
      },
      "source": [
        "bayesian_tuner.search(X_train, y_train,\n",
        "               epochs=3,\n",
        "               validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 9 Complete [00h 00m 21s]\n",
            "val_accuracy: 0.8266800045967102\n",
            "\n",
            "Best val_accuracy So Far: 0.875\n",
            "Total elapsed time: 00h 03m 05s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "FJcHC8d87mNt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a610cf67-6251-4976-9409-30a57ed02be9"
      },
      "source": [
        "bayesian_tuner.results_summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results summary\n",
            "Results in ./keras-tuner-trial/bayesian_optimization_4\n",
            "Showing 10 best trials\n",
            "Objective(name='val_accuracy', direction='max')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 512\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.875\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 352\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8678799867630005\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 480\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.860319972038269\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 256\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8562399744987488\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 352\n",
            "learning_rate: 0.01\n",
            "activation: sigmoid\n",
            "Score: 0.8393200039863586\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 512\n",
            "learning_rate: 0.01\n",
            "activation: sigmoid\n",
            "Score: 0.8347200155258179\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8316400051116943\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.83024001121521\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8275200128555298\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8268799781799316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woo9D9AU7mNu"
      },
      "source": [
        " ### Results\n",
        " \n",
        "Identify and write the the best performing hyperparameter combination and model score. \n",
        "Note that because this is  Bayesian Optimization, multiple runs might have slighly different outcomes. \n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "1badcdca408cdd49bc2e409dca3bac5a",
          "grade": true,
          "grade_id": "cell-ff95600bf745f40f",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "1EXa47mH7mNu"
      },
      "source": [
        "From bayesian_tuner.results_summary():\n",
        "\n",
        "Best Hyperparameter Combination:\n",
        "\n",
        "units: 512\n",
        "\n",
        "learning_rate: 0.001\n",
        "\n",
        "activation: relu\n",
        "\n",
        "This model resulted in a validation accuracy of 0.875"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOZ5-tJDraFE"
      },
      "source": [
        "We should point out that Gridsearch split the training set internally and performed 3-fold cross-validation whereas keras-tuner required a validation/test set. This means that the keras-tuner algorithms were using one validation set and our skearn GridSearchCV was using a different set - so this isn't a perfectly exact 1-to-1 comparision but it'll have to do. \n",
        "\n",
        "In order to compensate for this, we did score the best model on the same test set that keras-tuner used. Because the best-performing model from keras-tuner used our QuickDraw testing holdout data for validation, the test holdout is no longer valid as \"new, unseen data\". Information has leaked into our modeling process; namely, the best model was chosen based on the testing dataset. Ideally, keras-tuner should support cross-validation so that we do not need to create a separate validation holdout from our training data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPYChhrC7mNx"
      },
      "source": [
        "_______\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "The spirit of this experiment is to expose you to the idea of benchmarking and comparing the trade-offs of various gridsearch approaches. \n",
        "\n",
        "Even if we did find a way to pass in the original test set into GridSearchCV, we can see that both Random Search and Bayesian Optimization are arguably better alternatives to a brute force grid search when we consider the trade-offs of run time and locating the best performing model. "
      ]
    }
  ]
}